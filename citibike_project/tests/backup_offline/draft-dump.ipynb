{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ee35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD CODE --- IGNORE ---\n",
    "def download_file(url, download_dir=DOWNLOAD_DIR):\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    file_name = os.path.join(download_dir, os.path.basename(url))\n",
    "    print(file_name)\n",
    "    subprocess.run([\"wget\", \"-q\", \"-N\", \"-P\", download_dir, url], check=True)\n",
    "    subprocess.run([\"unzip\", \"-o\", file_name, \"-d\", download_dir], check=True)\n",
    "    unzip_dir = Path(f\"{download_dir}/unzipped_files\") # will use f\"{file_name.strip('.zip.csv')}\" in DAG to create cloud directory \n",
    "    archive_dir = Path(f\"{download_dir}/archive_files\") # TODO: remember use Pthlib or os.path to join paths for prod\n",
    "    # create directories to store unzipped and archived files\n",
    "    subprocess.run([\"mkdir\", \"-p\", unzip_dir], check=True)\n",
    "    subprocess.run([\"mkdir\", \"-p\", archive_dir], check=True)\n",
    "    # move unzipped and archived files to respective directories\n",
    "    unzipped_file = file_name.strip('.zip')\n",
    "    archive = file_name\n",
    "    subprocess.run([\"mv\", unzipped_file, unzip_dir], check=True)\n",
    "    subprocess.run([\"mv\", archive, archive_dir], check=True)\n",
    "    return file_name\n",
    "\n",
    "\n",
    "    def upload_data(**kwargs):\n",
    "    backend = os.environ.get(\"STORAGE_BACKEND\")\n",
    "\n",
    "    if backend == \"gcp\":\n",
    "        from google.cloud import storage\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(os.environ[\"GCP_BUCKET_NAME\"])\n",
    "        blob = bucket.blob(\"data/citi_bike.csv\")\n",
    "        blob.upload_from_filename(\"/tmp/citi_bike.csv\")\n",
    "\n",
    "    elif backend == \"azure\":\n",
    "        from azure.storage.blob import BlobServiceClient\n",
    "        blob_service = BlobServiceClient.from_connection_string(os.environ[\"AZURE_CONN_STR\"])\n",
    "        blob_client = blob_service.get_blob_client(container=os.environ[\"AZURE_CONTAINER_NAME\"],\n",
    "                                                   blob=\"data/citi_bike.csv\")\n",
    "        with open(\"/tmp/citi_bike.csv\", \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "\n",
    "@safe_run\n",
    "def download_files_wl(url, download_dir=DOWNLOAD_DIR):        \n",
    "    filecount = 0\n",
    "    # create directories to store unzipped and archived files\n",
    "\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    archive_dir = Path(f\"{download_dir}/archive_files\") \n",
    "    file_path = Path(archive_dir) / os.path.basename(url)\n",
    "    files_dir = str(os.path.basename(url)).strip('JC-citibike-tripdata.zip.csv')\n",
    "    unzip_dir = Path(f\"{download_dir}/unzipped_files/{files_dir}\")\n",
    "\n",
    "    # Download using subprocess and wget\n",
    "    #print(f\"Downloading {url} to {file_path}\")\n",
    "    subprocess.run([\"wget\", \"-q\", \"-N\", \"-P\", archive_dir, url], check=True)\n",
    "\n",
    "    # Extract depending on file type\n",
    "    if file_path.suffix == \".zip\":\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_dir)\n",
    "        #print(f\"Extracted ZIP to {unzip_dir}\")\n",
    "    elif file_path.suffix in [\".tar\", \".gz\", \".bz2\"]:\n",
    "        with tarfile.open(file_path, 'r:*') as tar_ref:\n",
    "            tar_ref.extractall(unzip_dir)\n",
    "        #print(f\"Extracted TAR to {unzip_dir}\")\n",
    "    else:\n",
    "        print(\"No extraction performed, unknown file type\")\n",
    "\n",
    "    \n",
    "    filecount += 1\n",
    "    print(f\"Download and extraction of {os.path.basename(url)} complete. Total files processed: {filecount}\")\n",
    "\n",
    "for url in files_list[-1:]:\n",
    "    download_files_wl(url)\n",
    "\n",
    "\n",
    "    \n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".asm\") or filename.endswith(\".py\"): \n",
    "        # print(os.path.join(directory, filename))\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "while True:\n",
    "        start_time = time()\n",
    "        \n",
    "        df = next(df_iter)\n",
    "        df.to_sql(name=\"citibike_data\", con=engine, if_exists=\"append\")\n",
    "\n",
    "        end_time = time()\n",
    "\n",
    "        print(f'inserted another chunk, ops took {(end_time-start_time):.2f}')\n",
    "\n",
    "\n",
    "# Create an iterator from the large dataset\n",
    "df_iter = pd.read_csv(path, iterator=True, chunksize=100000, parse_dates=[\"started_at\", \"ended_at\"])\n",
    "df = next(df_iter)\n",
    "\n",
    "# Create schema in psql database\n",
    "df.head(n=0).to_sql(name=\"citibike_data_202508\", con=engine, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "citibikebq_engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}', \n",
    "                       isolation_level='AUTOCOMMIT')\n",
    "\n",
    "try:\n",
    "    with citibikebq_engine.connect() as citibikebq_conn:\n",
    "        # Initialize BigQuery client\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/bonaventure/gcp-keys.json\"\n",
    "\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        # Download data from BigQuery and load to Postgres\n",
    "        for year in range(2013, 2015):  # TODO: Extend range until year 2019 in production or cloud environment\n",
    "            #print(year)\n",
    "            # Query BigQuery in 1 million row chunks\n",
    "            chunk_size = 1000000  # 1 million rows per chunk\n",
    "            offset = 0\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    query = f\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM `bigquery-public-data.new_york_citibike.citibike_trips`\n",
    "                    WHERE EXTRACT(YEAR FROM starttime) = {year}\n",
    "                    LIMIT {chunk_size} OFFSET {offset}\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    df_chunk = client.query(query).to_dataframe()\n",
    "                    if df_chunk.empty:\n",
    "                        logging.info(f\"Insertion into postgres db '{DB_NAME}' complete: %s\", \"citibike_trips_{year}\")\n",
    "                        break  # stop when there is no more data\n",
    "                    \n",
    "                    df_chunk.to_sql(f'citibike_trips_{year}', engine, if_exists='replace', index=False)\n",
    "                    logging.info(f\"Loaded rows {offset} to {offset + len(df_chunk)} into Postgres\")\n",
    "                    #print(f\"Loaded rows {offset} to {offset + len(df_chunk)} into Postgres\")\n",
    "\n",
    "                    offset += chunk_size\n",
    "                except StopIteration:\n",
    "                    logging.info(f\"Insertion into postgres db {DB_NAME} complete: citibike_trips_{year}; just check if last\")\n",
    "                    break\n",
    "except Exception as e:\n",
    "            logging.error(\"Data insertion from Big Query failed: %s\", e)\n",
    "            raise \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2eddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "        def ingest_from_bigquery_to_postgres(params=params, chunk_size=1000_000):\n",
    "            \"\"\"Ingest data from Big Query to Postgres in chunks\"\"\"\n",
    "\n",
    "            # Replace these with your PostgreSQL credentials\n",
    "            DB_USER = params.user\n",
    "            DB_PASS = params.password\n",
    "            DB_HOST = params.host\n",
    "            DB_PORT = params.port\n",
    "            DB_NAME = 'citibikebq'  # The database you want to create\n",
    "\n",
    "            # Connect to default database and create new database if not exists\n",
    "            default_engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/postgres', \n",
    "                                isolation_level='AUTOCOMMIT')\n",
    "            try:\n",
    "                # Execute CREATE DATABASE\n",
    "                with default_engine.connect() as default_conn:\n",
    "                    # Check if the database exists\n",
    "                    result = default_conn.execute(text(\"SELECT 1 FROM pg_database WHERE datname = :dbname\"), {\"dbname\": DB_NAME})\n",
    "                    exists = result.scalar()  # Returns None if no rows found\n",
    "                    \n",
    "                    if not exists:\n",
    "                        default_conn.execute(text(f\"CREATE DATABASE {DB_NAME}\"))\n",
    "                        #print(f\"Database '{DB_NAME}' created successfully!\")\n",
    "                        logging.info(f\"Database '{DB_NAME}' created successfully!\")\n",
    "                    else:\n",
    "                        #print(f\"Database '{DB_NAME}' already exists.\")\n",
    "                        logging.info(f\"Database '{DB_NAME}' already exists.\")\n",
    "            except Exception as e:\n",
    "                        logging.error(\"Data insertion from Big Query failed: %s\", e)\n",
    "                        raise \n",
    "            \n",
    "            # Connect to the newly created database and ingest data into postgres container\n",
    "            citibikebq_engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}', \n",
    "                            isolation_level='AUTOCOMMIT')\n",
    "\n",
    "            try:\n",
    "                with citibikebq_engine.connect() as citibikebq_conn:\n",
    "                    # Initialize BigQuery client\n",
    "                    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/bonaventure/gcp-keys.json\"\n",
    "\n",
    "                    client = bigquery.Client()\n",
    "\n",
    "                    # Download data from BigQuery and load to Postgres\n",
    "                    for year in range(2013, 2015):  # TODO: Extend range until year 2019 in production or cloud environment\n",
    "                        #print(year)\n",
    "                        # Query BigQuery in 1 million row chunks\n",
    "                        chunk_size = chunk_size  # 1 million rows per chunk\n",
    "                        offset = 0\n",
    "\n",
    "                        while True:\n",
    "                            try:\n",
    "                                query = f\"\"\"\n",
    "                                SELECT *\n",
    "                                FROM `bigquery-public-data.new_york_citibike.citibike_trips`\n",
    "                                WHERE EXTRACT(YEAR FROM starttime) = {year}\n",
    "                                LIMIT {chunk_size} OFFSET {offset}\n",
    "                                \"\"\"\n",
    "                                \n",
    "                                df_chunk = client.query(query).to_dataframe()\n",
    "                                if df_chunk.empty:\n",
    "                                    logging.info(f\"Insertion into postgres db '{DB_NAME}' complete: %s\", \"citibike_trips_{year}\")\n",
    "                                    break  # stop when there is no more data\n",
    "                                \n",
    "                                df_chunk.to_sql(f'citibike_trips_{year}', engine, if_exists='replace', index=False)\n",
    "                                logging.info(f\"Loaded rows {offset} to {offset + len(df_chunk)} into Postgres\")\n",
    "                                #print(f\"Loaded rows {offset} to {offset + len(df_chunk)} into Postgres\")\n",
    "\n",
    "                                offset += chunk_size\n",
    "                            except StopIteration:\n",
    "                                logging.info(f\"Insertion into postgres db {DB_NAME} complete: citibike_trips_{year}; just check if last\")\n",
    "                                break\n",
    "            except Exception as e:\n",
    "                        logging.error(\"Data insertion from Big Query failed: %s\", e)\n",
    "                        raise "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
