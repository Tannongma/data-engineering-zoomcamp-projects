{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e9c09b-7a2c-4a6e-8745-24079b2ee74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip -q\n",
    "#!pip install lxml sqlalchemy psycopg2-binary pandas -q\n",
    "#!pip install google-cloud google-cloud-bigquery -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f7dd28f-cdf3-4200-aba0-8f9e3c857333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inmport necessary libraries\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "import zipfile\n",
    "import tarfile\n",
    "from urllib.parse import urljoin\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "from google.cloud import storage, bigquery\n",
    "#from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Local imports\n",
    "#from safe_run import safe_run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cededb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://minio-service:9000\",\n",
    "    aws_access_key_id=\"admin\",\n",
    "    aws_secret_access_key=\"password\",\n",
    ")\n",
    "\n",
    "s3.create_bucket(Bucket=\"citibike-data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd070c43-776f-4c9b-bfd6-33ad32a8e16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6fe9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear existing handlers to avoid duplicates\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s | %(levelname)s | %(filename)s:%(lineno)d | %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140850a-64ed-4c33-a562-f17242eaec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_URL = \"https://s3.amazonaws.com/tripdata/\"\n",
    "#DOWNLOAD_DIR = \"/home/bonaventure/Documents/data_engineering/data-engineering-zoomcamp-projects/citibike_project/data/citibike_data\"\n",
    "DOWNLOAD_DIR = \"./data/citibike_data\"\n",
    "\n",
    "def scrape_citibike_files():\n",
    "    xml_index_url = BASE_URL\n",
    "    response = requests.get(xml_index_url)\n",
    "    links = list()\n",
    "    soup = BeautifulSoup(response.text, features=\"xml\")\n",
    "    xml_keys = soup.find_all('Key')\n",
    "\n",
    "    # Extract all download links\n",
    "    files = [urljoin(BASE_URL, str(link.contents[0])) for link in xml_keys if str(link.contents[0]).endswith('.zip')]\n",
    "    return files\n",
    "\n",
    "\n",
    "def download_files(url, download_dir=DOWNLOAD_DIR):\n",
    "                \n",
    "    filecount = 0\n",
    "    # create directories to store unzipped and archived files\n",
    "    logging.info(\"Starting download: %s\", url)\n",
    "    try:\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "        archive_dir = Path(f\"{download_dir}/archive_files\") \n",
    "        file_path = Path(archive_dir) / os.path.basename(url)\n",
    "        files_dir = str(os.path.basename(url)).strip('JC-citibike-tripdata.zip.csv')\n",
    "        unzip_dir = Path(f\"{download_dir}/unzipped_files/{files_dir}\")\n",
    "\n",
    "        # Download using subprocess and wget\n",
    "        #print(f\"Downloading {url} to {file_path}\")\n",
    "        subprocess.run([\"wget\", \"-q\", \"-N\", \"-P\", archive_dir, url], check=True)\n",
    "\n",
    "        logging.info(\"Download complete: %s\", file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"Download failed: %s\", e)\n",
    "        raise \n",
    "\n",
    "    # Extract depending on file type\n",
    "    if file_path.suffix == \".zip\":\n",
    "        try:\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(unzip_dir)\n",
    "            #print(f\"Extracted ZIP to {unzip_dir}\")\n",
    "            logging.info(\"zip file extraction complete: %s\", file_path)\n",
    "        except zipfile.BadZipFile:\n",
    "            logging.error(\"Invalid zip file: %s\", file_path)\n",
    "            raise\n",
    "    elif file_path.suffix in [\".tar\", \".gz\", \".bz2\"]:\n",
    "        try:\n",
    "            with tarfile.open(file_path, 'r:*') as tar_ref:\n",
    "                tar_ref.extractall(unzip_dir)\n",
    "            #print(f\"Extracted TAR to {unzip_dir}\")\n",
    "            logging.info(\"tar-like file extraction complete: %s\", file_path)\n",
    "        except tarfile.TarError:\n",
    "            logging.error(\"Invalid tar file: %s\", file_path)\n",
    "            raise\n",
    "    else:\n",
    "        #print(\"No extraction performed, unknown file type\")\n",
    "        logging.warning(\"Unknown file type, skipping extraction: %s\", file_path)\n",
    "    \n",
    "    filecount += 1\n",
    "    print(f\"Download and extraction of {os.path.basename(url)} complete. Total files processed: {filecount}\")\n",
    "\n",
    "\n",
    "def find_csv_file(DOWNLOAD_DIR=DOWNLOAD_DIR):\n",
    "    unzip_dir_list = os.listdir(f\"{DOWNLOAD_DIR}/unzipped_files\")\n",
    "    #print(unzip_dir_list)\n",
    "    paths_list = []\n",
    "\n",
    "    for dir in unzip_dir_list:\n",
    "        #print(dir)\n",
    "        folder = Path(f\"{DOWNLOAD_DIR}/unzipped_files/{dir}\")\n",
    "        for file in os.listdir(folder):\n",
    "            #print(str(file))\n",
    "            filename = str(file)\n",
    "            if filename.endswith(\".csv\"):\n",
    "                path = Path.joinpath(folder, filename)\n",
    "                #print(path)\n",
    "            elif filename.endswith(\".parquet\"): #TODO: handle parquet files later\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "            paths_list.append(path)\n",
    "            #print(paths_list)\n",
    "    return paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e78b37-ffef-4966-a9ae-f9e1d278b75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 19:08:52,256 | INFO | 1543903068.py:20 | Starting download: https://s3.amazonaws.com/tripdata/JC-202508-citibike-tripdata.csv.zip\n",
      "2025-09-20 19:08:53,013 | INFO | 1543903068.py:32 | Download complete: /home/bonaventure/Documents/data_engineering/data-engineering-zoomcamp-projects/2-docker_terraform/data/citibike_data/archive_files/JC-202508-citibike-tripdata.csv.zip\n",
      "2025-09-20 19:08:53,862 | INFO | 1543903068.py:44 | zip file extraction complete: /home/bonaventure/Documents/data_engineering/data-engineering-zoomcamp-projects/2-docker_terraform/data/citibike_data/archive_files/JC-202508-citibike-tripdata.csv.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download and extraction of JC-202508-citibike-tripdata.csv.zip complete. Total files processed: 1\n"
     ]
    }
   ],
   "source": [
    "files_list = scrape_citibike_files()\n",
    "\n",
    "for url in files_list[-1:]:\n",
    "    download_files(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a084cc85-fa11-4849-b6af-5c8cc626db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into cloud storage\n",
    "def upload_to_gcs(bucket_name, local_path, gcs_path):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(gcs_path)\n",
    "    blob.upload_from_filename(local_path)\n",
    "    print(f\"Uploaded {local_path} to gs://{bucket_name}/{gcs_path}\")\n",
    "\n",
    "def upload_to_aws(bucket_name, local_path, gcs_path): # TODO: Implement AWS upload\n",
    "    pass\n",
    "\n",
    "def upload_to_azure(bucket_name, local_path, gcs_path): # TODO: Implement Azure upload\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54b689b3-b7a9-48af-8842-779df795f036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AABD1C039D2D622D</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2025-08-18 09:02:16.100</td>\n",
       "      <td>2025-08-18 09:07:45.510</td>\n",
       "      <td>City Hall - Washington St &amp; 1 St</td>\n",
       "      <td>HB105</td>\n",
       "      <td>14 St Ferry - 14 St &amp; Shipyard Ln</td>\n",
       "      <td>HB202</td>\n",
       "      <td>40.737360</td>\n",
       "      <td>-74.030970</td>\n",
       "      <td>40.752961</td>\n",
       "      <td>-74.024353</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3420D743AD40EDC8</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2025-08-15 14:04:18.806</td>\n",
       "      <td>2025-08-15 14:29:45.333</td>\n",
       "      <td>Newark St &amp; Washington St</td>\n",
       "      <td>HB612</td>\n",
       "      <td>JC Medical Center</td>\n",
       "      <td>JC110</td>\n",
       "      <td>40.736810</td>\n",
       "      <td>-74.030900</td>\n",
       "      <td>40.715391</td>\n",
       "      <td>-74.049692</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6957013BE2AF6B52</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2025-08-23 13:56:08.239</td>\n",
       "      <td>2025-08-23 13:59:58.649</td>\n",
       "      <td>Exchange Pl</td>\n",
       "      <td>JC116</td>\n",
       "      <td>York St &amp; Marin Blvd</td>\n",
       "      <td>JC097</td>\n",
       "      <td>40.716366</td>\n",
       "      <td>-74.034344</td>\n",
       "      <td>40.716615</td>\n",
       "      <td>-74.042412</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B005E728E67ED43F</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2025-08-19 17:41:53.820</td>\n",
       "      <td>2025-08-19 17:50:22.265</td>\n",
       "      <td>Riverview Park</td>\n",
       "      <td>JC057</td>\n",
       "      <td>14 St Ferry - 14 St &amp; Shipyard Ln</td>\n",
       "      <td>HB202</td>\n",
       "      <td>40.744319</td>\n",
       "      <td>-74.043991</td>\n",
       "      <td>40.752961</td>\n",
       "      <td>-74.024353</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2BF0B8E6F05BC2AF</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2025-08-27 09:26:19.273</td>\n",
       "      <td>2025-08-27 09:31:24.381</td>\n",
       "      <td>Manila &amp; 1st</td>\n",
       "      <td>JC082</td>\n",
       "      <td>JC Medical Center</td>\n",
       "      <td>JC110</td>\n",
       "      <td>40.721651</td>\n",
       "      <td>-74.042884</td>\n",
       "      <td>40.715391</td>\n",
       "      <td>-74.049692</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type              started_at  \\\n",
       "0  AABD1C039D2D622D  electric_bike 2025-08-18 09:02:16.100   \n",
       "1  3420D743AD40EDC8   classic_bike 2025-08-15 14:04:18.806   \n",
       "2  6957013BE2AF6B52   classic_bike 2025-08-23 13:56:08.239   \n",
       "3  B005E728E67ED43F  electric_bike 2025-08-19 17:41:53.820   \n",
       "4  2BF0B8E6F05BC2AF   classic_bike 2025-08-27 09:26:19.273   \n",
       "\n",
       "                 ended_at                start_station_name start_station_id  \\\n",
       "0 2025-08-18 09:07:45.510  City Hall - Washington St & 1 St            HB105   \n",
       "1 2025-08-15 14:29:45.333         Newark St & Washington St            HB612   \n",
       "2 2025-08-23 13:59:58.649                       Exchange Pl            JC116   \n",
       "3 2025-08-19 17:50:22.265                    Riverview Park            JC057   \n",
       "4 2025-08-27 09:31:24.381                      Manila & 1st            JC082   \n",
       "\n",
       "                    end_station_name end_station_id  start_lat  start_lng  \\\n",
       "0  14 St Ferry - 14 St & Shipyard Ln          HB202  40.737360 -74.030970   \n",
       "1                  JC Medical Center          JC110  40.736810 -74.030900   \n",
       "2               York St & Marin Blvd          JC097  40.716366 -74.034344   \n",
       "3  14 St Ferry - 14 St & Shipyard Ln          HB202  40.744319 -74.043991   \n",
       "4                  JC Medical Center          JC110  40.721651 -74.042884   \n",
       "\n",
       "     end_lat    end_lng member_casual  \n",
       "0  40.752961 -74.024353        member  \n",
       "1  40.715391 -74.049692        casual  \n",
       "2  40.716615 -74.042412        member  \n",
       "3  40.752961 -74.024353        member  \n",
       "4  40.715391 -74.049692        member  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore a sample file\n",
    "path = find_csv_file()[0]\n",
    "citibike_data_202508 = pd.read_csv(filepath_or_buffer=path, \n",
    "                                   #nrows=100,\n",
    "                                   parse_dates=[\"started_at\", \"ended_at\"])\n",
    "citibike_data_202508.head()\n",
    "#citibike_data_202508.count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c3d5337-c1b2-42f3-957c-58864c58bcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x7bdc1f80f400>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to postgres database inside containter\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/citibike\")\n",
    "db_connection = engine.connect()\n",
    "db_connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b65382b0-a37b-41a8-95a7-92cb108155c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE citibike_data (\n",
      "\tride_id TEXT, \n",
      "\trideable_type TEXT, \n",
      "\tstarted_at TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tended_at TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tstart_station_name TEXT, \n",
      "\tstart_station_id TEXT, \n",
      "\tend_station_name TEXT, \n",
      "\tend_station_id TEXT, \n",
      "\tstart_lat FLOAT(53), \n",
      "\tstart_lng FLOAT(53), \n",
      "\tend_lat FLOAT(53), \n",
      "\tend_lng FLOAT(53), \n",
      "\tmember_casual TEXT\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = pd.io.sql.get_schema(citibike_data_202508, name=\"citibike_data\", con=db_connection)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4b4ef85-0cac-49dc-99e4-d2610f179203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_to_postgres(paths_list=find_csv_file(), engine=engine, data_path=path):\n",
    "    \"\"\"Create schema in psql database and load data\"\"\"\n",
    "    for path in paths_list[-1:]:\n",
    "        df_name = \"_\".join([\"citibike\", str(path).split(\"/\")[-2].strip()])\n",
    "        # Create an iterator from the large dataset\n",
    "        df_header = pd.read_csv(filepath_or_buffer=path,\n",
    "                              parse_dates=[\"started_at\", \"ended_at\"]).head(n=0)\n",
    "            \n",
    "        try:\n",
    "            # Load the header of the df as schemas\n",
    "            df_header.to_sql(name=df_name, con=engine, if_exists=\"replace\")\n",
    "            # Create an iterator from the large dataset\n",
    "            df_iter = pd.read_csv(filepath_or_buffer=path,\n",
    "                                chunksize=500000, \n",
    "                                parse_dates=[\"started_at\", \"ended_at\"])\n",
    "            while True:\n",
    "                try:\n",
    "                    start_time = time()\n",
    "                    chunk_num = 0\n",
    "                    df = next(df_iter)\n",
    "                    df.to_sql(name=df_name, con=engine, if_exists=\"append\")\n",
    "                    chunk_num += 1\n",
    "                    end_time = time()\n",
    "                except StopIteration:\n",
    "                    logging.info(f\"Finished ingesting chunck {chunk_num} into postgres; just check if last\")\n",
    "                    break\n",
    "            print(f'Insertion of {df_name} complete, I/O osp time {(end_time-start_time):.2f}')\n",
    "            logging.info(f\"Insertion into postgres db complete: %s\", df_name)\n",
    "        except Exception as e:\n",
    "            logging.error(\"Data insertion failed: %s\", e)\n",
    "            raise \n",
    "\n",
    "#load_data_to_postgres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01478c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'user': 'postgres',\n",
    "            'password': 'postgres',\n",
    "            'host': 'localhost',\n",
    "            'port': '5432'\n",
    "     }\n",
    "\n",
    "def ingest_from_bigquery_to_postgres(params=params):\n",
    "    \"\"\"Ingest data from Big Query to Postgres in chunks\"\"\"\n",
    "\n",
    "    # Replace these with your PostgreSQL credentials\n",
    "    #DB_USER = params.user\n",
    "    #DB_PASS = params.password\n",
    "    #DB_HOST = params.host\n",
    "    #DB_PORT = params.port\n",
    "    #DB_NAME = 'citibikebq'  # The database you want to create\n",
    "\n",
    "    # Use these for a dictionary input for nb testing\n",
    "    DB_USER = params['user']\n",
    "    DB_PASS = params['password']\n",
    "    DB_HOST = params['host']\n",
    "    DB_PORT = params['port']\n",
    "    DB_NAME = 'citibikebq'  # The database you want to create\n",
    "\n",
    "    # Connect to default database and create new database if not exists\n",
    "    default_engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/postgres', \n",
    "                        isolation_level='AUTOCOMMIT')\n",
    "    try:\n",
    "        # Execute CREATE DATABASE\n",
    "        with default_engine.connect() as default_conn:\n",
    "            # Check if the database exists\n",
    "            result = default_conn.execute(text(\"SELECT 1 FROM pg_database WHERE datname = :dbname\"), {\"dbname\": DB_NAME})\n",
    "            exists = result.scalar()  # Returns None if no rows found\n",
    "            \n",
    "            if not exists:\n",
    "                default_conn.execute(text(f\"CREATE DATABASE {DB_NAME}\"))\n",
    "                #print(f\"Database '{DB_NAME}' created successfully!\")\n",
    "                logging.info(f\"Database '{DB_NAME}' created successfully!\")\n",
    "            else:\n",
    "                #print(f\"Database '{DB_NAME}' already exists.\")\n",
    "                logging.info(f\"Database '{DB_NAME}' already exists.\")\n",
    "\n",
    "    except Exception as e:\n",
    "                logging.error(\"Data insertion from Big Query failed: %s\", e)\n",
    "                raise \n",
    "    \n",
    "    # Connect to the newly created database and ingest data into postgres container\n",
    "    citibikebq_engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}', \n",
    "                       isolation_level='AUTOCOMMIT')\n",
    "\n",
    "    try:\n",
    "        with citibikebq_engine.connect() as citibikebq_conn:\n",
    "            # Initialize BigQuery client\n",
    "            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/bonaventure/gcp-keys.json\"\n",
    "\n",
    "            client = bigquery.Client()\n",
    "\n",
    "            # Download data from BigQuery and load to Postgres\n",
    "            for year in [2013]:#range(2013, 2015):  # TODO: Extend range until year 2019 in production or cloud environment\n",
    "                # Query BigQuery in 1 million row chunks\n",
    "                chunk_size = 500_000  # 1/2 million rows per chunk\n",
    "                offset = 0\n",
    "\n",
    "                # Check if table exists\n",
    "                check_num = 0\n",
    "                if check_num >= 20:  # Safety check to avoid infinite loops during testing\n",
    "                    logging.warning(\"Reached maximum number of checks, stopping to avoid infinite loop.\")\n",
    "                    break\n",
    "\n",
    "                check_table_query = f\"SELECT to_regclass('public.citibike_trips_{year}')\"\n",
    "                check_table_result = citibikebq_conn.execute(text(check_table_query))\n",
    "                table_exists = check_table_result.scalar()\n",
    "\n",
    "                while True:\n",
    "                    try:\n",
    "                        # Skip table if exists\n",
    "                        if table_exists:\n",
    "                            print(f\"Table 'citibike_trips_{year}' already exists in {DB_NAME}\")\n",
    "                            check_num += 1\n",
    "                            break\n",
    "                        # Query BigQuery in chunks\n",
    "                        query = f\"\"\"\n",
    "                        SELECT *\n",
    "                        FROM `bigquery-public-data.new_york_citibike.citibike_trips`\n",
    "                        WHERE EXTRACT(YEAR FROM starttime) = {year}\n",
    "                        LIMIT {chunk_size} OFFSET {offset}\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        df_chunk = client.query(query).to_dataframe()\n",
    "                        if df_chunk.empty:\n",
    "                            logging.info(f\"Insertion into postgres db '{DB_NAME}' complete: %s\", \"citibike_trips_{year}\")\n",
    "                            break  # stop when there is no more data\n",
    "                        \n",
    "                        df_chunk.to_sql(f'citibike_trips_{year}', \n",
    "                                        citibikebq_engine, \n",
    "                                        if_exists='replace', index=False)\n",
    "                        logging.info(f\"Loaded rows {offset} to {offset + len(df_chunk)} into Postgres\")\n",
    "                        #print(f\"Loaded rows {offset} to {offset + len(df_chunk)} into Postgres\")\n",
    "\n",
    "                        offset += chunk_size\n",
    "                    except StopIteration:\n",
    "                        logging.info(f\"Insertion into postgres db {DB_NAME} complete for table citibike_trips_{year}; just check if last\")\n",
    "                        break\n",
    "    except Exception as e:\n",
    "                logging.error(\"Data insertion from Big Query failed: %s\", e)\n",
    "                raise \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "767d38b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 20:57:38,209 | INFO | 3505170162.py:41 | Database 'citibikebq' already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'citibike_trips_2013' already exists in citibikebq\n"
     ]
    }
   ],
   "source": [
    "ingest_from_bigquery_to_postgres()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
