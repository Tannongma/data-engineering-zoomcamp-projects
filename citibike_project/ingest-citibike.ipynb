{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e9c09b-7a2c-4a6e-8745-24079b2ee74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip -q\n",
    "#!pip install lxml sqlalchemy psycopg2-binary pandas -q\n",
    "#!pip install google-cloud google-cloud-bigquery -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f7dd28f-cdf3-4200-aba0-8f9e3c857333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inmport necessary libraries\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import logging\n",
    "import zipfile\n",
    "import tarfile\n",
    "import gzip\n",
    "from urllib.parse import urljoin\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "from google.cloud import storage, bigquery\n",
    "#from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Local imports\n",
    "#from safe_run import safe_run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cededb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method ClientCreator._create_api_method.<locals>._api_call of <botocore.client.S3 object at 0x70f069884d60>>\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://localhost:9000\",\n",
    "    aws_access_key_id=\"minio\",\n",
    "    aws_secret_access_key=\"password\",\n",
    ")\n",
    "\n",
    "#s3.create_bucket(Bucket=\"citibike-data\")\n",
    "bucket_list = s3.list_buckets\n",
    "\n",
    "\n",
    "print(bucket_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd070c43-776f-4c9b-bfd6-33ad32a8e16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6fe9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear existing handlers to avoid duplicates\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s | %(levelname)s | %(filename)s:%(lineno)d | %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3140850a-64ed-4c33-a562-f17242eaec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_URL = \"https://s3.amazonaws.com/tripdata/\"\n",
    "#DOWNLOAD_DIR = \"/home/bonaventure/Documents/data_engineering/data-engineering-zoomcamp-projects/citibike_project/data/citibike_data\"\n",
    "DOWNLOAD_DIR = \"./data/citibike_data\"\n",
    "TEST_DOWNLOAD_DIR = \"./tests/data\"\n",
    "\n",
    "def scrape_citibike_files():\n",
    "    xml_index_url = BASE_URL\n",
    "    response = requests.get(xml_index_url)\n",
    "    links = list()\n",
    "    soup = BeautifulSoup(response.text, features=\"xml\")\n",
    "    xml_keys = soup.find_all('Key')\n",
    "\n",
    "    # Extract all download links\n",
    "    files = [urljoin(BASE_URL, str(link.contents[0])) for link in xml_keys if str(link.contents[0]).endswith('.zip')]\n",
    "    return files\n",
    "\n",
    "\n",
    "def download_files(url, download_dir=DOWNLOAD_DIR):\n",
    "                \n",
    "    filecount = 0\n",
    "    # create directories to store unzipped and archived files\n",
    "    logging.info(\"Starting download: %s\", url)\n",
    "    try:\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "        archive_dir = Path(f\"{download_dir}/archive_files\") \n",
    "        file_path = Path(archive_dir) / os.path.basename(url)\n",
    "        files_dir = str(os.path.basename(url)).strip('JC-citibike-tripdata.zip.csv')\n",
    "        unzip_dir = Path(f\"{download_dir}/unzipped_files/{files_dir}\")\n",
    "\n",
    "        # Download using subprocess and wget\n",
    "        #print(f\"Downloading {url} to {file_path}\")\n",
    "        subprocess.run([\"wget\", \"-q\", \"-N\", \"-P\", archive_dir, url], check=True)\n",
    "\n",
    "        logging.info(\"Download complete: %s\", file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"Download failed: %s\", e)\n",
    "        raise \n",
    "\n",
    "    # Extract depending on file type\n",
    "    if file_path.suffix == \".zip\":\n",
    "        try:\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(unzip_dir)\n",
    "            #print(f\"Extracted ZIP to {unzip_dir}\")\n",
    "            logging.info(\"zip file extraction complete: %s\", file_path)\n",
    "        except zipfile.BadZipFile:\n",
    "            logging.error(\"Invalid zip file: %s\", file_path)\n",
    "            raise\n",
    "    elif file_path.suffix in [\".tar\", \".gz\", \".bz2\"]:\n",
    "        try:\n",
    "            with tarfile.open(file_path, 'r:*') as tar_ref:\n",
    "                tar_ref.extractall(unzip_dir)\n",
    "            #print(f\"Extracted TAR to {unzip_dir}\")\n",
    "            logging.info(\"tar-like file extraction complete: %s\", file_path)\n",
    "        except tarfile.TarError:\n",
    "            logging.error(\"Invalid tar file: %s\", file_path)\n",
    "            raise\n",
    "    elif file_path.suffix == \".csv\":\n",
    "        try:\n",
    "            subprocess.run([\"mv\", \"-f\", \"-t\", unzip_dir, file_path], check=True)\n",
    "            #print(f\"Extracted ZIP to {unzip_dir}\")\n",
    "            logging.warning(f\"Found and moved csv file to {unzip_dir} directory: %s\", file_path)\n",
    "        except zipfile.BadZipFile:\n",
    "            logging.error(\"Invalid zip file: %s\", file_path)\n",
    "            raise\n",
    "    else:\n",
    "        #print(\"No extraction performed, unknown file type\")\n",
    "        logging.warning(\"Unknown file type, skipping extraction: %s\", file_path)\n",
    "    \n",
    "    filecount += 1\n",
    "    print(f\"Download and extraction of {os.path.basename(url)} complete. Total files processed: {filecount}\")\n",
    "\n",
    "\n",
    "def find_files(DOWNLOAD_DIR=DOWNLOAD_DIR):\n",
    "    unzip_dir_list = os.listdir(f\"{DOWNLOAD_DIR}/unzipped_files\")\n",
    "    #print(unzip_dir_list)\n",
    "    paths_dict = {}\n",
    "    csv_paths_list = []\n",
    "    gzip_paths_list = []\n",
    "\n",
    "    for dir in unzip_dir_list:\n",
    "        #print(dir)\n",
    "        folder = Path(f\"{DOWNLOAD_DIR}/unzipped_files/{dir}\")\n",
    "        for file in os.listdir(folder):\n",
    "            #print(str(file))\n",
    "            filename = str(file)\n",
    "            if filename.endswith(\".csv\"):\n",
    "                path = Path.joinpath(folder, filename) # WARNING: Returns PosixPath and might be converted to str\n",
    "                csv_paths_list.append(path) \n",
    "                #print(path)\n",
    "            elif filename.endswith(\".parquet\"): #TODO: handle parquet files later\n",
    "                continue\n",
    "            elif filename.endswith(\".gz\"): #TODO: handle Gzip files later\n",
    "                path = Path.joinpath(folder, filename) # WARNING: Returns PosixPath and might be converted to str\n",
    "                gzip_paths_list.append(path)           \n",
    "            else:\n",
    "                continue\n",
    "            paths_dict[\"csv_paths_list\"] = csv_paths_list\n",
    "            paths_dict[\"gzip_paths_list\"] = gzip_paths_list\n",
    "            #print(paths_list)\n",
    "    return paths_dict\n",
    "\n",
    "\n",
    "def gzip_csv_files(csv_files_list = find_files()[\"csv_paths_list\"]):\n",
    "        \"\"\"Gzip csv files to save space\"\"\"\n",
    "        for path in csv_files_list:\n",
    "            with open(path, 'rb') as f_in:\n",
    "                with gzip.open(f\"{path}.gz\", 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            os.remove(path)  # Remove the original CSV file after compression\n",
    "            logging.info(\"Gzipped file created: %s.gz\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4ce5eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:25:32,714 | INFO | 2033080096.py:22 | Starting download: http://localhost:8000/Downloads/JC-202508-citibike-tripdata.csv\n",
      "2025-09-28 18:25:32,869 | INFO | 2033080096.py:34 | Download complete: /home/bonaventure/Documents/data_engineering/data-engineering-zoomcamp-projects/citibike_project/tests/data/archive_files/JC-202508-citibike-tripdata.csv\n",
      "2025-09-28 18:25:33,133 | WARNING | 2033080096.py:63 | Found and moved csv file to /home/bonaventure/Documents/data_engineering/data-engineering-zoomcamp-projects/citibike_project/tests/data/unzipped_files/202508 directory: /home/bonaventure/Documents/data_engineering/data-engineering-zoomcamp-projects/citibike_project/tests/data/archive_files/JC-202508-citibike-tripdata.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download and extraction of JC-202508-citibike-tripdata.csv complete. Total files processed: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'csv_paths_list': [PosixPath('/home/bonaventure/Documents/data_engineering/data-engineering-zoomcamp-projects/citibike_project/tests/data/unzipped_files/202508/JC-202508-citibike-tripdata.csv')],\n",
       " 'gzip_paths_list': []}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://localhost:8000/Downloads/JC-202508-citibike-tripdata.csv\"\n",
    "\n",
    "\n",
    "\n",
    "download_files(url, download_dir=TEST_DOWNLOAD_DIR)\n",
    "find_files(DOWNLOAD_DIR=TEST_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ccff8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_to_minio(url, download_dir=TEST_DOWNLOAD_DIR):\n",
    "                \n",
    "    filecount = 0\n",
    "    # create directories to store unzipped and archived files\n",
    "    logging.info(\"Starting download to MinIO: %s\", url)\n",
    "    try:\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "        archive_dir = Path(f\"{download_dir}/archive_files\") \n",
    "        file_path = Path(archive_dir) / os.path.basename(url)\n",
    "        files_dir = str(os.path.basename(url)).strip('JC-citibike-tripdata.zip.csv')\n",
    "        unzip_dir = Path(f\"{download_dir}/unzipped_files/{files_dir}\")\n",
    "        os.makedirs(unzip_dir, exist_ok=True)\n",
    "\n",
    "        # Download using subprocess and wget\n",
    "        #print(f\"Downloading {url} to {file_path}\")\n",
    "        subprocess.run([\"wget\", \"-q\", \"-N\", \"-P\", archive_dir, url], check=True)\n",
    "\n",
    "        logging.info(\"Download complete: %s\", file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"Download failed: %s\", e)\n",
    "        raise \n",
    "\n",
    "    # Extract depending on file type\n",
    "    if file_path.suffix == \".zip\":\n",
    "        try:\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(unzip_dir)\n",
    "            #print(f\"Extracted ZIP to {unzip_dir}\")\n",
    "            logging.info(\"zip file extraction complete: %s\", file_path)\n",
    "        except zipfile.BadZipFile:\n",
    "            logging.error(\"Invalid zip file: %s\", file_path)\n",
    "            raise\n",
    "    elif file_path.suffix in [\".tar\", \".gz\", \".bz2\"]:\n",
    "        try:\n",
    "            with tarfile.open(file_path, 'r:*') as tar_ref:\n",
    "                tar_ref.extractall(unzip_dir)\n",
    "            #print(f\"Extracted TAR to {unzip_dir}\")\n",
    "            logging.info(\"tar-like file extraction complete: %s\", file_path)\n",
    "        except tarfile.TarError:\n",
    "            logging.error(\"Invalid tar file: %s\", file_path)\n",
    "            raise\n",
    "    elif file_path.suffix == \".csv\":\n",
    "        try:\n",
    "            subprocess.run([\"mv\", \"-f\", \"-t\", unzip_dir, file_path], check=True)\n",
    "            #print(f\"Extracted ZIP to {unzip_dir}\")\n",
    "            logging.warning(f\"Found and moved csv file to {unzip_dir} directory: %s\", file_path)\n",
    "        except zipfile.BadZipFile:\n",
    "            logging.error(\"Invalid zip file: %s\", file_path)\n",
    "            raise\n",
    "    else:\n",
    "        #print(\"No extraction performed, unknown file type\")\n",
    "        logging.warning(\"Unknown file type, skipping extraction: %s\", file_path)\n",
    "    \n",
    "    filecount += 1\n",
    "    print(f\"Download and extraction of {os.path.basename(url)} complete. Total files processed: {filecount}\")\n",
    "\n",
    "    csv_files_list = find_files(DOWNLOAD_DIR=download_dir)[\"csv_paths_list\"]\n",
    "    gzip_csv_files(csv_files_list=csv_files_list)\n",
    "\n",
    "    gzip_paths_list = find_files(DOWNLOAD_DIR=download_dir)[\"gzip_paths_list\"]\n",
    "    uploaded_files_list = []\n",
    "    # WARNING: Returns PosixPath and might be converted to str\n",
    "    for file in gzip_paths_list:\n",
    "        if file not in uploaded_files_list:\n",
    "            s3.upload_file(Filename=str(file), Key=str(file), Bucket=\"citibike-data\")\n",
    "            uploaded_files_list.append(file)\n",
    "\n",
    "    print(f\"Gzip archieving and upload of {os.path.basename(url)} to MinIO complete. Total files processed: {filecount}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c159853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:46:46,858 | INFO | 3163766360.py:5 | Starting download to MinIO: http://localhost:8000/Downloads/JC-202508-citibike-tripdata.csv\n",
      "2025-09-28 18:46:47,256 | INFO | 3163766360.py:18 | Download complete: tests/data/archive_files/JC-202508-citibike-tripdata.csv\n",
      "2025-09-28 18:46:47,267 | WARNING | 3163766360.py:47 | Found and moved csv file to tests/data/unzipped_files/202508 directory: tests/data/archive_files/JC-202508-citibike-tripdata.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download and extraction of JC-202508-citibike-tripdata.csv complete. Total files processed: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 18:46:50,815 | INFO | 1674881534.py:112 | Gzipped file created: tests/data/unzipped_files/202508/JC-202508-citibike-tripdata.csv.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gzip archieving and upload of JC-202508-citibike-tripdata.csv complete. Total files processed: 1\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost:8000/Downloads/JC-202508-citibike-tripdata.csv\"\n",
    "download_files_to_minio(url=url, download_dir=TEST_DOWNLOAD_DIR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98e78b37-ffef-4966-a9ae-f9e1d278b75e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /tripdata/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x70f061dab190>: Failed to resolve 's3.amazonaws.com' ([Errno -3] Temporary failure in name resolution)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/socket.py:967\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    966\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    968\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/urllib3/connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/urllib3/connection.py:753\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x70f061dab190>: Failed to resolve 's3.amazonaws.com' ([Errno -3] Temporary failure in name resolution)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /tripdata/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x70f061dab190>: Failed to resolve 's3.amazonaws.com' ([Errno -3] Temporary failure in name resolution)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m files_list \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_citibike_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m files_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m      4\u001b[0m     download_files(url)\n",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m, in \u001b[0;36mscrape_citibike_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_citibike_files\u001b[39m():\n\u001b[1;32m      7\u001b[0m     xml_index_url \u001b[38;5;241m=\u001b[39m BASE_URL\n\u001b[0;32m----> 8\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_index_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     links \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     10\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/analytics_env/lib/python3.10/site-packages/requests/adapters.py:677\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    674\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /tripdata/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x70f061dab190>: Failed to resolve 's3.amazonaws.com' ([Errno -3] Temporary failure in name resolution)\"))"
     ]
    }
   ],
   "source": [
    "files_list = scrape_citibike_files()\n",
    "\n",
    "for url in files_list[-1:]:\n",
    "    download_files(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a084cc85-fa11-4849-b6af-5c8cc626db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into cloud storage\n",
    "def upload_to_gcs(bucket_name, local_path, gcs_path):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(gcs_path)\n",
    "    blob.upload_from_filename(local_path)\n",
    "    print(f\"Uploaded {local_path} to gs://{bucket_name}/{gcs_path}\")\n",
    "\n",
    "def upload_to_aws(bucket_name, local_path, gcs_path): # TODO: Implement AWS upload\n",
    "    pass\n",
    "\n",
    "def upload_to_azure(bucket_name, local_path, gcs_path): # TODO: Implement Azure upload\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54b689b3-b7a9-48af-8842-779df795f036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/citibike_data/unzipped_files/202508/JC-202508-citibike-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "# Explore a sample file\n",
    "path = find_files()[\"csv_paths_list\"][0]\n",
    "citibike_data_202508 = pd.read_csv(filepath_or_buffer=path, \n",
    "                                   #nrows=100,\n",
    "                                   parse_dates=[\"started_at\", \"ended_at\"])\n",
    "citibike_data_202508.head()\n",
    "#citibike_data_202508.count()\n",
    "\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c3d5337-c1b2-42f3-957c-58864c58bcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x70f069976fb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to postgres database inside containter\n",
    "\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/citibike\")\n",
    "db_connection = engine.connect()\n",
    "db_connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b65382b0-a37b-41a8-95a7-92cb108155c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE citibike_data (\n",
      "\tride_id TEXT, \n",
      "\trideable_type TEXT, \n",
      "\tstarted_at TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tended_at TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tstart_station_name TEXT, \n",
      "\tstart_station_id TEXT, \n",
      "\tend_station_name TEXT, \n",
      "\tend_station_id TEXT, \n",
      "\tstart_lat FLOAT(53), \n",
      "\tstart_lng FLOAT(53), \n",
      "\tend_lat FLOAT(53), \n",
      "\tend_lng FLOAT(53), \n",
      "\tmember_casual TEXT\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = pd.io.sql.get_schema(citibike_data_202508, name=\"citibike_data\", con=db_connection)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4b4ef85-0cac-49dc-99e4-d2610f179203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_to_postgres(paths_list=find_files()[\"csv_paths_list\"], engine=engine, data_path=path):\n",
    "    \"\"\"Create schema in psql database and load data\"\"\"\n",
    "    for path in paths_list[-1:]:\n",
    "        df_name = \"_\".join([\"citibike\", str(path).split(\"/\")[-2].strip()])\n",
    "        # Create an iterator from the large dataset\n",
    "        df_header = pd.read_csv(filepath_or_buffer=path,\n",
    "                              parse_dates=[\"started_at\", \"ended_at\"]).head(n=0)\n",
    "            \n",
    "        try:\n",
    "            # Load the header of the df as schemas\n",
    "            df_header.to_sql(name=df_name, con=engine, if_exists=\"replace\")\n",
    "            # Create an iterator from the large dataset\n",
    "            df_iter = pd.read_csv(filepath_or_buffer=path,\n",
    "                                chunksize=500000, \n",
    "                                parse_dates=[\"started_at\", \"ended_at\"])\n",
    "            while True:\n",
    "                try:\n",
    "                    start_time = time()\n",
    "                    chunk_num = 0\n",
    "                    df = next(df_iter)\n",
    "                    df.to_sql(name=df_name, con=engine, if_exists=\"append\")\n",
    "                    chunk_num += 1\n",
    "                    end_time = time()\n",
    "                except StopIteration:\n",
    "                    logging.info(f\"Finished ingesting chunck {chunk_num} into postgres; just check if last\")\n",
    "                    break\n",
    "            print(f'Insertion of {df_name} complete, I/O osp time {(end_time-start_time):.2f}')\n",
    "            logging.info(f\"Insertion into postgres db complete: %s\", df_name)\n",
    "        except Exception as e:\n",
    "            logging.error(\"Data insertion failed: %s\", e)\n",
    "            raise \n",
    "\n",
    "#load_data_to_postgres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcea063",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = \"data/citibike_data/unzipped_files/202508/JC-202508-citibike-tripdata.csv\"\n",
    "s3.upload_file(Filename=file, Key=file, Bucket=\"citibike-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01478c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'user': 'postgres',\n",
    "            'password': 'postgres',\n",
    "            'host': 'localhost',\n",
    "            'port': '5432'\n",
    "     }\n",
    "\n",
    "def ingest_from_bigquery_to_postgres(params=params):\n",
    "    \"\"\"Ingest data from Big Query to Postgres in chunks\"\"\"\n",
    "\n",
    "    # Replace these with your PostgreSQL credentials\n",
    "    #DB_USER = params.user\n",
    "    #DB_PASS = params.password\n",
    "    #DB_HOST = params.host\n",
    "    #DB_PORT = params.port\n",
    "    #DB_NAME = 'citibikebq'  # The database you want to create\n",
    "\n",
    "    # Use these for a dictionary input for nb testing\n",
    "    DB_USER = params['user']\n",
    "    DB_PASS = params['password']\n",
    "    DB_HOST = params['host']\n",
    "    DB_PORT = params['port']\n",
    "    DB_NAME = 'citibikebq'  # The database you want to create\n",
    "\n",
    "    # Connect to default database and create new database if not exists\n",
    "    default_engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/postgres', \n",
    "                        isolation_level='AUTOCOMMIT')\n",
    "    try:\n",
    "        # Execute CREATE DATABASE\n",
    "        with default_engine.connect() as default_conn:\n",
    "            # Check if the database exists\n",
    "            result = default_conn.execute(text(\"SELECT 1 FROM pg_database WHERE datname = :dbname\"), {\"dbname\": DB_NAME})\n",
    "            exists = result.scalar()  # Returns None if no rows found\n",
    "            \n",
    "            if not exists:\n",
    "                default_conn.execute(text(f\"CREATE DATABASE {DB_NAME}\"))\n",
    "                #print(f\"Database '{DB_NAME}' created successfully!\")\n",
    "                logging.info(f\"Database '{DB_NAME}' created successfully!\")\n",
    "            else:\n",
    "                #print(f\"Database '{DB_NAME}' already exists.\")\n",
    "                logging.info(f\"Database '{DB_NAME}' already exists.\")\n",
    "\n",
    "    except Exception as e:\n",
    "                logging.error(\"Data insertion from Big Query failed: %s\", e)\n",
    "                raise \n",
    "    \n",
    "    # Connect to the newly created database and ingest data into postgres container\n",
    "    citibikebq_engine = create_engine(f'postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}', \n",
    "                       isolation_level='AUTOCOMMIT')\n",
    "\n",
    "    try:\n",
    "        with citibikebq_engine.connect() as citibikebq_conn:\n",
    "            # Initialize BigQuery client\n",
    "            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/bonaventure/gcp-keys.json\"\n",
    "\n",
    "            client = bigquery.Client()\n",
    "\n",
    "            # Download data from BigQuery and load to Postgres\n",
    "            for year in [2013]:#range(2013, 2015):  # TODO: Extend range until year 2019 in production or cloud environment\n",
    "                # Query BigQuery in 1 million row chunks\n",
    "                chunk_size = 500_000  # 1/2 million rows per chunk\n",
    "                offset = 0\n",
    "\n",
    "                # Check if table exists\n",
    "                check_num = 0\n",
    "                if check_num >= 20:  # Safety check to avoid infinite loops during testing\n",
    "                    logging.warning(\"Reached maximum number of checks, stopping to avoid infinite loop.\")\n",
    "                    break\n",
    "\n",
    "                check_table_query = f\"SELECT to_regclass('public.citibike_trips_{year}')\"\n",
    "                check_table_result = citibikebq_conn.execute(text(check_table_query))\n",
    "                table_exists = check_table_result.scalar()\n",
    "\n",
    "                while True:\n",
    "                    try:\n",
    "                        # Skip table if exists\n",
    "                        if table_exists:\n",
    "                            print(f\"Table 'citibike_trips_{year}' already exists in {DB_NAME}\")\n",
    "                            check_num += 1\n",
    "                            break\n",
    "                        # Query BigQuery in chunks\n",
    "                        query = f\"\"\"\n",
    "                        SELECT *\n",
    "                        FROM `bigquery-public-data.new_york_citibike.citibike_trips`\n",
    "                        WHERE EXTRACT(YEAR FROM starttime) = {year}\n",
    "                        LIMIT {chunk_size} OFFSET {offset}\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        df_chunk = client.query(query).to_dataframe()\n",
    "                        if df_chunk.empty:\n",
    "                            logging.info(f\"Insertion into postgres db '{DB_NAME}' complete: %s\", \"citibike_trips_{year}\")\n",
    "                            break  # stop when there is no more data\n",
    "                        \n",
    "                        df_chunk.to_sql(f'citibike_trips_{year}', \n",
    "                                        citibikebq_engine, \n",
    "                                        if_exists='replace', index=False)\n",
    "                        logging.info(f\"Loaded rows {offset} to {offset + len(df_chunk)} into Postgres\")\n",
    "                        #print(f\"Loaded rows {offset} to {offset + len(df_chunk)} into Postgres\")\n",
    "\n",
    "                        offset += chunk_size\n",
    "                    except StopIteration:\n",
    "                        logging.info(f\"Insertion into postgres db {DB_NAME} complete for table citibike_trips_{year}; just check if last\")\n",
    "                        break\n",
    "    except Exception as e:\n",
    "                logging.error(\"Data insertion from Big Query failed: %s\", e)\n",
    "                raise \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "767d38b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 20:57:38,209 | INFO | 3505170162.py:41 | Database 'citibikebq' already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'citibike_trips_2013' already exists in citibikebq\n"
     ]
    }
   ],
   "source": [
    "ingest_from_bigquery_to_postgres()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
